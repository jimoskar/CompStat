---
title: "TMA4300: Exercise 1"
author: "Jim Totland, Martin Tufte"
date: "1/29/2022"
output: pdf_document
fig_width: 3
fig_height: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 5, fig.height = 4, fig.align = "center")
library(tidyverse)
library(pracma)
set.seed(4300)
```



# Problem A
## A.1
The exponential distribution has a cumulative density function given by

$$
F(x) = 1 - e^{-\lambda x},
$$

where $\lambda$ is the rate parameter. By defining $u := F(x)$, $x$ can be expressed as 

$$
x = -\frac{1}{\lambda}\ln(1-u) =: F^{-1}(u).
$$
This means that we can use the \textit{inversion method} to simulate from the exponential distribution. Let $U \sim \mathcal{U}_{[0,1]}$ and calculate $X = F^{-1}(U)$, then $X \sim \mathrm{Exp}(\lambda)$. A function which simulates from the exponential distribution is given below.

```{r}
sim.exp <- function(rate, n){
  # Vector of n uniform values
  u <- runif(n, 0, 1)
  # Note that (1-U) ~ U, so we can use u instead of 1-u in the returned value.
  -1/rate * log(u)
}
```
Next, we need to check if this gives reasonable results by comparing our simulated values to the theoretical knowledge using $\lambda = 0.5$ and $n = 1000$.
```{r, cache=TRUE}
rate <- 0.5
n <- 10000
sim <- data.frame(x = sim.exp(rate, n))
x = seq(from = 0, to = 15, by = 0.1)
exact <- data.frame(x = x, y = rate*exp(-rate*x))

ggplot(sim) + 
  geom_histogram(aes(x = x, y = ..density.., fill = "Simulated"),
                 alpha = 0.8, binwidth = 0.1) + 
  geom_line(data = exact , aes(x = x, y = y, color = 'Exact'))  +
  scale_color_manual(name = "", values = c("Exact" = "#e0474c")) +
  scale_fill_manual(name = "", values = c("Simulated" = "#69b3a2")) 
```
We also compare the estimated mean and variance to first and second central moments of the exponential distribution:

```{r}
# Calculate the cumulative mean
data.frame(n.samples = 1:n,
           mean = cumsum(sim$x)/(1:n)) %>%
  ggplot() + geom_line(aes(n.samples, mean)) +
  geom_hline(yintercept = 1/rate, color = "red") +
  ggtitle("Mean")
  
# Calculate the cumulative variance
data.frame(n.samples = 1:n,
           mean = cumsum(sim$x)/(1:n),
           mean2 = cumsum(sim$x^2)/(1:n)) %>%
  mutate(var = mean2-mean^2) %>%
  ggplot() + geom_line(aes(n.samples, var)) +
  geom_hline(yintercept = 1/rate^2, color = "red") +
  ggtitle("Variance")
```
We observe that the the sampled mean and variance approach the theoretical values as the number of samples grows larger. 


## A.2

We consider the probability distribution given by
$$
g(x) = 
\begin{cases}
c x^{\alpha-1}, & 0 < x < 1\\
ce^{-x}, & 1 \le x\\
0, & \text{otherwise}
\end{cases},
$$
where $c$ is a normalizing constant and $\alpha \in (0,\ 1)$.

### a)
Normalizing the density using $\int_{-\infty}^{\infty} g(x)\ \text{d}x = c \int_0^1 x^{\alpha-1}\ \text{d}x + c\int_1^{\infty} e^{-x}\ \text{d}x = c(\frac1\alpha + \frac1e) = 1$, it follows that the normalizing constant is $c = (\frac1\alpha + \frac1e)^{-1}$.
The cumulative distribution function is given by
$$
G(x) = \int_{-\infty}^x g(y)\ \text{d}y =
\begin{cases}
c \large\int_0^x y^{\alpha-1}\ \text{d}y, & 0 < x < 1 \\
\dfrac{c}{\alpha} + c \large\int_1^{x} e^{-y}\ \text{d}y, & 1 \le x \\
0, & \text{otherwise}
\end{cases},
$$
which evaluates to
$$
G(x) =
\begin{cases}
\dfrac{cx^\alpha}{\alpha}, & 0 < x < 1 \\
1 - c e^{-x}, & 1 \le x \\
0, & \text{otherwise}
\end{cases}.
$$
The inverse is the unique function $G^{-1}$ such that $x = G^{-1}(u)$. Since $G(x)$ is piecewise continuous and monotonic, we can calculate the inverses for its constituencies and find appropriate intervals. The intervals are simply found from calculating the corresponding boundary points. In $G(x)$ one of these points is located at $x=1$, meaning $G(1) = \frac{c}{\alpha}$ will separate two continuous parts in $G^{-1}$. After solving for $u$, we end up with the inverse given by
$$
G^{-1}(u) =
\begin{cases}
\left(\dfrac{\alpha u}{c}\right)^{-\alpha}, & 0 \le u < \dfrac{c}{\alpha} \\
-\ln\left(\dfrac{1-u}{c}\right), & \dfrac{c}{\alpha} \le u
\end{cases}.
$$

### b)
Firstly, we implement $g$ and an algorithm to generate samples from $g$:
```{r}
g <- function(alpha, x){
  # Normalizing constant
  c <- (1/alpha + exp(-1))^-1
  c*x^(alpha-1) * (0<x)*(x<1) + c*exp(-x) *(x>=1)
} 

sim.g <- function(alpha, n){
  # Normalizing constant
  c <- (1/alpha + exp(-1))^-1
  # Vector of n uniform values
  u <- runif(n, 0, 1)
  
  (alpha/c * u)^(1/alpha) * (u < c/alpha) - log((1-u)/c) * (c/alpha <= u)
}
```

```{r, echo = FALSE}
# Implemented for testing purposes

g <- function(alpha, x){
  # Normalizing constant
  c <- (1/alpha + exp(-1))^-1
  c*x^(alpha-1) * (0<x)*(x<1) + c*exp(-x) *(x>=1)
} 

G <- function(alpha, x){
  # Normalizing constant
  c <- (1/alpha + exp(-1))^-1
  c/alpha*x^alpha * (0<x)*(x<1) + (1 - c*exp(-x)) *(x>=1)
} 

G.inv <- function(alpha, u){
  # Normalizing constant
  c <- (1/alpha + exp(-1))^-1
  
  (alpha/c * u)^(1/alpha) * (u < c/alpha) - log((1-u)/c) * (u >= c/alpha)
}
```

To test our implementation we use $\alpha = 0.5$ and simulate $n=10000$ samples.

```{r}
alpha <- 0.5
n <- 10000
sim <- data.frame(x = sim.g(alpha, n))
x = seq(from = 0.002, to = 5, by = 0.05)
exact <- data.frame(x = x, y = g(alpha, x))

ggplot(sim) + 
  geom_histogram(aes(x = x, y = ..density.., fill = "Simulated"),
                 alpha = 0.8, binwidth = 0.02) + 
  geom_line(data = exact , aes(x = x, y = y, color = 'Exact'))  +
  scale_color_manual(name = "", values = c("Exact" = "#e0474c")) +
  scale_fill_manual(name = "", values = c("Simulated" = "#69b3a2")) + coord_cartesian(xlim=c(0, 5))
```
From the histogram, we see that we are able to generate samples from $g$ corresponding to the true density. Next we find the theoretical mean and variance and compare with the estimated values. The theoretical mean is given by
$$
\mu_g := \int_{-\infty}^{\infty} x\ g(x)\ \text{d}x = c \int_{0}^1 x^{\alpha}\ \text{d}x + c \int_{1}^{\infty} x e^{-x}\ \text{d}x = \frac{c}{\alpha+1} + \frac{2c}{e}.
$$
Similarly, we find the variance
$$
\begin{split}
\text{Var}_g & := \int_{-\infty}^{\infty} x^2 f(x)\ \text{d}x - \mu_g^2 \\
& = c \int_0^1 x^{\alpha + 1}\ \text{d}x + c \int_{1}^{\infty} x^2 e^{-x}\ \text{d}x - \mu_g^2\\
& = \frac{c}{\alpha+2} + \frac{5c}{e} - \left(\frac{c}{\alpha+1} + \frac{2c}{e}\right)^2.
\end{split}
$$
Comparing the true mean and variance with estimated values: \textcolor{red}{ikke plott her?}
```{r}
alpha <- 0.5
c <- (1/alpha + exp(-1))^-1

# 10000 realizations from simulation
sim <- sim.g(alpha, 10000)

# Theoretical mean and variance
mean.g <- c/(alpha+1) + 2*c/exp(1)
var.g <- c/(alpha+2) + 5*c/exp(1) - mean.g^2

# Estimated mean
mean.est <- mean(sim)
var.est <- var(sim)

# Comparing means
c(mean.g, mean.est)

# Comparing variance
c(var.g, var.est)
```
We see that our simulation gives approximate correct values for the theoretical mean and variance.


## A.3
### a)
To find the value of $c$, we integrate the density over the entire domain and equate the result to 1:
$$
1 = \int_{-\infty}^{\infty}f(x)dx = c\int_{-\infty}^{\infty}\frac{e^{\alpha x}}{(1 + e^{\alpha x})^2}dx.
$$
To progress from here, we introduce the substitution, $v = e^{\alpha x}$, which gives

$$
1 = \frac{c}{\alpha} \int_0^{\infty} \frac{dv}{(1 + v)^2} = \frac{c}{\alpha}(-\frac{1}{1 + v})\Big|_0^\infty =\frac{c}{\alpha}.
$$
Consequently, $c = \alpha$.

### b)
The CDF is defined as follows,
$$
\begin{split}
F(x) &= \int_{-\infty}^xf(z)dz =  \int_0^{\exp(\alpha x)} \frac{dv}{(1 + v)^2} \\
 &= 1-\frac{1}{1 + \exp(\alpha x)} = \frac{\exp(\alpha x)}{1 + \exp(\alpha x)},
\end{split}
$$

where we have used the same substitution as earlier, namely $v = e^{\alpha z}$. We notice that $F(x)$ is the Sigmoid function, which has the well known logit-function as its inverse. I.e.,
$$
F^{-1}(x) = \frac{1}{\alpha}\ln\left(\frac{x}{1-x}\right).
$$

### c)

Since we have an analytic expression for the inverse, we can again use the \textit{inversion method} to sample from $f$. The sampling-function is given as follows.

```{r}
sim.sigm <- function(alpha, n){
  u <- runif(n,0,1)
  1/alpha * log(u/(1-u))
}
```

```{r, cache = TRUE}
alpha <- 2
n <- 10000
sim <- data.frame(x = sim.sigm(alpha, n))
x = seq(from = -10, to = 10, by = 0.1)
exact <- data.frame(x = x, y = alpha*exp(alpha*x)/(1 + exp(alpha*x))^2)

ggplot(sim) + 
  geom_histogram(aes(x = x, y = ..density.., fill = "Simulated"),
                 alpha = 0.8, binwidth = 0.1) + 
  geom_line(data = exact , aes(x = x, y = y, color = 'Exact'))  +
  scale_color_manual(name = "", values = c("Exact" = "#e0474c")) +
  scale_fill_manual(name = "", values = c("Simulated" = "#69b3a2"))
```

To compare the simulated values with the theoretical mean and variance, we first need to compute the expression of these moments. \textcolor{red}{Think maybe the mean is undefined??}

## A.4
Below is our implementation of the Box-Muller algorithm.
```{r}
box.mul <- function(n){
  odd <-  FALSE
  if(n %% 2 != 0){
    odd <-  TRUE
    n <- n + 1
  }
  
  x1 <- 2*pi * runif(n/2, 0, 1)
  x2 <- sim.exp(0.5, n/2)
  
  y1 <- sqrt(x2)*cos(x1)
  y2 <- sqrt(x2)*sin(x1)
  
  concat <- c(y1,y2)
  if(odd){
    return(head(concat, -1))
  }
  else{
    return(concat)
  }
}
```
As usual, we compare the results of a simulation against the theoretical distribution.
```{r, cache=TRUE}
n <- 10000
sim.box.mul <- data.frame(x = box.mul(n))
x = seq(from = -5, to = 5, by = 0.1)
exact <- data.frame(x = x, y = 1/sqrt(2*pi)*exp(-1/2*x^2))

ggplot(sim.box.mul) + 
  geom_histogram(aes(x = x, y = ..density.., fill = "Simulated"),
                 alpha = 0.8, binwidth = 0.1) + 
  geom_line(data = exact , aes(x = x, y = y, color = 'Exact'))  +
  scale_color_manual(name = "", values = c("Exact" = "#e0474c")) +
  scale_fill_manual(name = "", values = c("Simulated" = "#69b3a2"))
```
Finally, we compare the mean and variance resulting from the simulation with the theoretical values. 
```{r}
# Mean
data.frame(n.samples = 1:n,
           mean = cumsum(sim.box.mul$x)/(1:n)) %>%
  ggplot() + geom_line(aes(n.samples, mean)) +
  geom_hline(yintercept = 0, color = "red") +
  ggtitle("Mean")
  
# Variance
data.frame(n.samples = 1:n,
           mean = cumsum(sim.box.mul$x)/(1:n),
           mean2 = cumsum(sim.box.mul$x^2)/(1:n)) %>%
  mutate(var = mean2-mean^2) %>%
  ggplot() + geom_line(aes(n.samples, var)) +
  geom_hline(yintercept = 1, color = "red") +
  ggtitle("Variance")
```


## A.5

Let $\mathcal{N}_d(\mu, \Sigma)$ be a $d$-variate normal distribution with mean vector $\mu$ and covariance matrix $\Sigma$. This is our target distribution. To simulate from it, let $x \sim \mathcal{N}_d(0,I_d)$ be a standard $d$-variate normal distribution. Then, it follows that
$$
y = \mu + A x \quad \implies \quad y \sim \mathcal{N}(\mu, AA^T),
$$
where $A$ is a $d \times d$ matrix. Since the covariance matrix $\Sigma$ is both symmetric and positive-definite, we can write $\Sigma = A A^T$ using the Cholesky decomposition. Thus, for a given $\mu$ and $\Sigma$, we are able to simulate from this distribution after simulating from $\mathcal{N}_d(0,I_d)$.

Reusing our implementation of the Box-Muller algorithm from A.4, a function for generating samples from the multivariate normal distribution is shown below:
```{r}
sim.mvnorm <- function(mu, Sigma, n){
  # Returns n realizations from a d-variate normal distribution
  d <- length(mu)
  
  # Get n d-variate standard normal realizations using the Box-Muller algorithm
  x <- array(box.mul(n*d), dim=c(d,n))
  
  # Cholesky decomposition of Sigma
  A <- t(chol(Sigma)) # Transposes to get a lower triangular matrix
  
  mu + A %*% x
}
```

To test our implementation, we generate a bivariate normal distribution:
```{r}
# Bivariate example
mu <- c(1,2)
Sigma <- matrix(c(1, -.7, -.7, 2), 2)

# Simulate 10000 realizations
y <- sim.mvnorm(mu, Sigma, 10000)

# Plot first 500 points
plot(y[1,1:500], y[2,1:500], main="Bivariate normal distribution", xlab="x", ylab="y", pch=16)
```
Comparing the true mean and covariance with estimated values:
```{r}
# Estimate mean
mu.est <- rowMeans(y)
mu.est

# Estimate covariance matrix
Sigma.est <- cov(t(y))
Sigma.est
```
From the printout in R, we see that the estimated values are close to the true values $\mu = \begin{pmatrix}1 \\ 2\end{pmatrix}$ and $\Sigma = \begin{pmatrix}1 & -0.7 \\ -0.7 & 2\end{pmatrix}$. We conclude that our function works properly.


\newpage
# Problem B
## B.1
### (a)

## B.2
### (a)
We repeat that 
$$
f^*(x) = \begin{cases} x^{\alpha-1}e^{-x}, \quad &x>0, \\
0, \quad &\text{otherwise}.
\end{cases}
$$
To find $a = \sqrt{\sup_{x\ge 0} f^*(x)}$, we first note that $f^*(x = 0) = 0$ and only consider
$$
\sup_{x > 0} f^*(x),
$$
which amounts to solving

$$
\begin{split}
\frac{d}{dx} f^*(x)= 0 \\
&\iff (\alpha-1)x^{\alpha -2}e^{-x} - x^{\alpha -1}e^{-x} = 0 \\
&\iff x=\alpha-1>0,
\end{split}
$$
which clearly is a global maximum. Consequently,
$$
a = \sqrt{\alpha - 1}.
$$

Analogously, to find  $b_+ = \sqrt{\sup_{x\ge 0} x^2f^*(x)} = \sqrt{\sup_{x\ge 0}g(x)}$, we note that $g(z = 0) = 0$ and only consider $\sup_{x > 0}g(x)$, which amounts to solving

$$
\begin{split}
\frac{d}{dx} g(x)= 0 \\
&\iff (\alpha+1)x^{\alpha}e^{-x} - x^{\alpha + 1}e^{-x} = 0 \\
&\iff x=\alpha + 1>0,
\end{split},
$$
which clearly is a global maximum for $x \ge 0$. Consequently,
$$
b_+ = \sqrt{\alpha + 1}.
$$

Finally, $b_{-} = - \sqrt{\sup_{x \le 0} x^2f^*(x)} = - \sqrt{0} = 0$.

### (b)

```{r}
rou.gamma <- function(n, alpha){
  a <-  sqrt(alpha - 1)
  b.minus <- 0
  b.plus <- sqrt(alpha + 1)
  
  count <-  0
  tries <- 0
  result <- rep(0,n)
  
  while(count < n){
    x1 <- a*runif(1, 0, 1)
    x2 <- b.minus + b.plus*runif(1, 0, 1) 
  
    if(log(x1) <= 0.5*((alpha - 1)*log(x2/x1) - x2/x1)){
      result[count + 1] = x2/x1
      count <- count + 1
    }
    tries = tries + 1
  }
  return(list("sim" = result, "tries" = tries))
}
```

First, we check that the simulation algorithm gives reasonable results:
```{r, cache=TRUE}
n <- 10000
alpha <-  2

sim.gamma <- rou.gamma(n, alpha)
x = seq(from = 0.1, to = 12, by = 0.1)
exact <- data.frame(x = x, y = 1/gamma(alpha) * x^(alpha - 1) * exp(-x))

ggplot(data.frame(x = sim.gamma$sim)) + 
geom_histogram(aes(x = x, y = ..density.., fill = "Simulated"),
                 alpha = 0.8, binwidth = 0.1) + 
  geom_line(data = exact , aes(x = x, y = y, color = 'Exact'))  +
  scale_color_manual(name = "", values = c("Exact" = "#e0474c")) +
  scale_fill_manual(name = "", values = c("Simulated" = "#69b3a2"))

```

```{r}
n <- 1000
alpha.vec <- seq(1.01, 2000, length.out = 10)
tries.vec <- rep(0, 10)
for(i in 1:length(alpha.vec)){
  alpha <- alpha.vec[i]
  sim.gamma <- rou.gamma(n, alpha)
  tries.vec[i] = sim.gamma$tries
}

plot(alpha.vec, tries.vec, pch = 16)
abline(h = 3, col="blue")
```
The number of necessary tries is highest for low values of $\alpha$, i.e. $\ln\alpha < 2$, while it stabilizes after that. This indicates that $C_f$ constitutes a smaller proportion of $[0, a] \times [b_{-},b_{+}]$. \textcolor{red}{mer tolkning?? Tror det er feil. Tror antall tries skal øke med alpha.}

## B.3
We recall that the gamma distribution has probability density function (PDF)

$$
f(x) = \frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha -1}e^{-\beta x}.
$$
We use Marsaglia and Tsang’s method to generate from the gamma distribution:
```{r}
marsa.tsang <- function(n, alpha, beta){
  d <- alpha - 1/3
  c <- 1/sqrt(9*d)
  count <- 0
  result <- rep(0, n)
  while(count < n){
    z <- box.mul(1)
    u <- runif(1, 0, 1)
    v <- (1 + c*z)^3
    if(z > -1/c && log(u) < 0.5*z^2 + d - d*v + d*log(v)){
      result[count + 1] = d*v
      count <- count + 1
    }
  }
  return(1/beta * result)
}
```

Next, we check if our simulation algorithm gives sensible results.
```{r, cache=TRUE}
n <- 10000
alpha <-  2
beta <- 2

sim.gamma <- marsa.tsang(n, alpha, beta)
x = seq(from = 0.1, to = 12, by = 0.1)
exact <- data.frame(x = x, y = beta^alpha/gamma(alpha) * x^(alpha - 1) * exp(-beta*x))

ggplot(data.frame(x = sim.gamma)) + 
geom_histogram(aes(x = x, y = ..density.., fill = "Simulated"),
                 alpha = 0.8, binwidth = 0.05) + 
  geom_line(data = exact , aes(x = x, y = y, color = 'Exact'))  +
  scale_color_manual(name = "", values = c("Exact" = "#e0474c")) +
  scale_fill_manual(name = "", values = c("Simulated" = "#69b3a2"))
```



## B.4
We repeat that $X \sim \text{Gamma}(\alpha, 1)$ and  $Y \sim \text{Gamma}(\beta, 1)$, and note that their joint density is $f_{X,Y}(x,y) = f_X(x)f_Y(y)$, since they are independent. Next, we define $Z = \frac{X}{X + Y}$ and also introduce $V = X + Y$. Then, $X = ZV$ and $Y=V(1-Z)$, and, by the transformation formula, the joint density of $Z$ and $V$ is

$$
\begin{split}
f_{Z, V}(z,v) &= f_{X, Y}(zv, v(1-z)) \cdot \begin{vmatrix} v & z \\
-v & 1-z\end{vmatrix} \\
&= (\Gamma(\alpha) \Gamma(\beta))^{-1}\cdot(zv)^{\alpha-1} e^{-zv} \cdot (v(1-z))^{\beta -1}e^{-v(1-z)}\cdot[v(1-z) + vz] \\
&=(\Gamma(\alpha) \Gamma(\beta))^{-1} \cdot z^{\alpha-1}(1-z)^{\beta - 1}\cdot v^{[\alpha + \beta] - 1} \cdot e^{-v}.
\end{split}
$$
Thus, the marginal distribution of $Z$ is given as

$$
\begin{split}
\int_0^\infty f_{Z, V}(z,v)dv &= \Gamma(\alpha) \Gamma(\beta))^{-1} \cdot z^{\alpha-1}(1-z)^{\beta - 1} \int_0^\infty v^{[\alpha + \beta] - 1} \cdot e^{-v} dv \\
&= \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} z^{\alpha-1}(1-z)^{\beta - 1},
\end{split}
$$
which is what we wanted show.

\newpage
# Problem C
## C.1
We observe that $\theta = \mathrm{Pr}\{X > 4\} = \int_4^{\infty}f(x) = \int_{-\infty}^{\infty}\mathbb{I}(x > 4) f(x) = \mathrm{E}_f[\mathbb{I}(X >4)]$. Then, by the method of Monte Carlo (MC) integration, an estimate of $\theta$ is

$$
\hat{\theta} = \frac{1}{n}\sum_{i = 1}^n\mathbb{I}(X_i > 4).
$$
Here, $n$ is the number of samples, $\mathbb{I}$ is the indicator function and $X \sim \mathcal{N}(0, 1)$. The estimate is computed below.
```{r}
n <- 100000
x <- box.mul(n)
h <- (x > 4)
theta.hat <- 1/n * sum(h)
theta.hat
pnorm(4, lower.tail = FALSE)
```

To compute a confidence interval, we need the variance of $\hat\theta$. We define $h(X) = \mathbb{I}(X  >4 )$
$$
\begin{split}
\mathrm{Var}[\hat\theta] &= \frac{1}{n^2}\sum_{i = 1}^n\mathrm{Var}_f[h(X_i)] = \frac{1}{n}\mathrm{Var}_f[h(X)] \\
&\approx \frac{1}{n}\left( \frac{1}{n-1} \sum_{i = 1}^n (h(x_i) - \hat\theta)^2 \right) =: \hat{\sigma}^2.
\end{split}
$$
We also utilize the central limit theorem, such that $\hat\theta \sim \mathcal{N}\left(\theta, \mathrm{Var}[\hat\theta]\right)$. Then we know that

$$
\frac{\hat{\theta} - \theta}{\hat{\sigma}} \sim t_{n-1}.
$$
Consequently, a 95% confidence interval is given by

$$
\left[\hat{\theta} - t_{n-1, 0.025} \cdot \hat\sigma, \;\hat{\theta} + t_{n-1, 0.025} \cdot \hat\sigma \right].
$$
It is computed and displayed below.
```{r}
sigma.hat <- sqrt(1/(n*(n-1)) * sum((h - theta.hat)^2)) 
1/sigma.hat^2
lower <- theta.hat - qt(0.975, n - 1)*sigma.hat
upper <- theta.hat + qt(0.975, n - 1)*sigma.hat

c(lower, upper)
```

## C.2
To simulate from $g$, we need to find the normalization constant:
$$
1 = \int_4^{\infty}g(x)dx = -c\exp\left(-\frac{x^2}{2}\right)\Big|_4^{\infty} \iff c= \exp(8).
$$
The CDF, $G(x)$, is thus given as
$$
G(x) = \int_4^{x}g(t)dt = 1 - \exp\left(-\frac{x^2}{2} - 8\right).
$$
It can easily be shown that $G^{-1}(x) = \sqrt{16-2\ln(1-x)}$, which we utilize in the inversion sampling algorithm below. Sanity check:
```{r}
inv.samp <- function(n){
  u <- runif(n, 0, 1)
  x <- sqrt(16 - 2*log(1 - u))
  return(x)
}

sim <- inv.samp(10000)
x = seq(from = 4, to = 9, by = 0.1)
exact <- data.frame(x = x, y = exp(8)*x*exp(-x^2/2))

ggplot(data.frame(x = sim)) + 
 geom_histogram(aes(x = x, y = ..density.., fill = "Simulated"),
                 alpha = 0.8, binwidth = 0.01) + 
  geom_line(data = exact , aes(x = x, y = y, color = 'Exact'))  +
  scale_color_manual(name = "", values = c("Exact" = "#e0474c")) +
  scale_fill_manual(name = "", values = c("Simulated" = "#69b3a2"))
```
We note that $g(x) >0$ where $h(x)f(x) >0$ \textcolor{red}{show this??}. Then, in the importance sampling scheme, the estimator is given as
$$
\hat\theta_{IS} = \frac{1}{n} \sum_{i = 1}^n\frac{h(X_i)f(X_i)}{g(X_i)} = \frac{1}{n} \sum_{i = 1}^nh(X_i)w(X_i)
$$

We also note that $w(x) = \left(\sqrt{2\pi}cx\right)^{-1}$ Below we perform the importance sampling
```{r}
n <- 100000
c <- exp(8)

x <- inv.samp(n)
w <- 1/(sqrt(2*pi)*c*x)
h <- (x > 4)

theta.is <- 1/n * sum(h*w)
theta.is
```
To create a confidence interval, we need to estimate the variance.

$$
\begin{split}
\mathrm{Var}[\hat\theta_{IS}] &= \frac{1}{n^2}\sum_{i = 1}^n\mathrm{Var}_g[h(X_i)w(X_i)] = \frac{1}{n}\mathrm{Var}_g[h(X)w(X)] \\
&\approx \frac{1}{n}\left( \frac{1}{n-1} \sum_{i = 1}^n (h(x_i)w(x_i) - \hat\theta_{IS})^2 \right) =: \hat{\sigma}^2_{IS}.
\end{split}
$$
We use the same procedure as above, which leads to the confidence interval
$$
\left[\hat{\theta}_{IS} - t_{n-1, 0.025} \cdot \hat\sigma_{IS}, \;\hat{\theta}_{IS} + t_{n-1, 0.025} \cdot \hat\sigma_{IS} \right].
$$
It is computed numerically below.
```{r}
sigma.is <- sqrt(1/(n*(n-1)) * sum((h*w - theta.is)^2)) 
lower <- theta.is - qt(0.975, n - 1)*sigma.is
upper <- theta.is + qt(0.975, n - 1)*sigma.is

c(lower, upper)
1/sigma.is^2
```
We define the precision of the estimators as the reciprocal of the variance of the estimators. Hence we have $\text{Precision}( \hat{\theta})$ = `r 1/(sigma.hat^2)` and $\text{Precision}(\hat{\theta}_{IS})$ =  `r 1/(sigma.is^2)`. \textcolor{red}{unsure how to compare them with respect to n...}

## C.3
### (a)
The algorithm which samples $n$ antithetic from $g$ is given below.
```{r}
anti <- function(n){
  u <- runif(n, 0, 1)
  q <- c(u, 1 - u)
  x <- sqrt(16 - 2*log(1 - q))
  return(x)
}
```
### (b)
Below, we generate $n = 50000$ pairs of antithetic variates which we use in the importance sampling. We choose $n$ as such because we generate pairs and $2n = 100000$, which is what we used in C.2.
```{r}
n <- 100000
c <- exp(8)

x <- anti(n/2)
w <- 1/(sqrt(2*pi)*c*x)
h <- (x > 4)

theta.a <- 1/n * sum(h*w)
theta.a
```
Finally, we compute a 95% confidence interval:
```{r}
sigma.a <- sqrt(1/(n*(n-1)) * sum((h*w - theta.a)^2)) 
lower <- theta.a - qt(0.975, n - 1)*sigma.a
upper <- theta.a + qt(0.975, n - 1)*sigma.a

c(lower, upper)
1/sigma.a^2
```
We observe that the precision is lower than for regular importance sampling.



\newpage
# Problem D

## 1.
We consider a multinomial distribution with mass function given by
$$
f(y \mid \theta) \propto (2+\theta)^{y_1}(1-\theta)^{y_2+y_3}\theta^{y_4},
$$
where $\theta \in (0,1)$. Using a prior $g(\theta)$ on $(0,1)$, the observed posterior density is $f(\theta\mid y) \propto f(y \mid\theta)\ g(\theta)$. First, let $g(\theta) = \mathcal{U}_{[0,1]}$, i.e. the uniform distribution, which is equivalent to a $\text{Beta}(1,1)$. Then the observed posterior density becomes
$$
f(\theta\mid y) \propto f(y \mid\theta)\ \mathcal{U}_{[0,1]} \propto \underbrace{(2+\theta)^{y_1}(1-\theta)^{y_2+y_3}\theta^{y_4}}_{\tilde f(\theta \mid y)} \qquad \text{for}\quad \theta \in (0,1).
$$
We can now use rejection sampling to sample from $f(\theta \mid y)$ using the improper density $\tilde f(\theta\mid y)$. For the rejection sampling to work optimally, meaning we want highest possible acceptance rate, it is beneficial to scale the density such that $\text{max}_\theta\ \frac{\tilde f(\theta \mid y)}{\tilde c\ g(\theta)} = 1$, where $\tilde c$ is a constant. When $g(\theta) = \mathcal{U}_{[0,1]}$, the maximum of $\tilde f(\theta\mid y)$ can be found from maximizing its logarithm. Deriving $\ln \tilde f(\theta\mid y)$ with respect to $\theta$ and setting equal to zero, we obtain
$$
\frac{\text{d}\ \ln \tilde f(\theta\mid y)}{\text{d}\theta} = \frac{y_1}{2+\theta} - \frac{y_2+y_3}{1-\theta} + \frac{y_4}{\theta} = 0,
$$
which on the interval $(0,1)$ has a maximum at
$$
\theta^* = \frac{15+\sqrt{53809}}{394} \approx 0.6268,
$$
where we used the values for $\{y_i\}_{i=1}^4$ given in the problem description. Next we use this to find the constant $\tilde c = \tilde f(\theta^*\mid y) \approx \exp(67.38)$.

### Rejection sampling algorithm
1. Sample $\theta$ from the prior distribution $g(\theta)$.
2. Sample $u$ from the uniform distribution $\mathcal{U}_{[0,1]}$.
3. Compute $\alpha = \frac{1}{\tilde c} \frac{\tilde f(\theta)}{g(\theta)}$.
4. If $u \le \alpha$: keep $\theta$. Otherwise do nothing.

```{r}
# Calculating c
y <- c(125, 18, 20, 34) # From project description
theta.star <- (15 + sqrt(53809))/394
c <- exp(y[1]*log(2+theta.star) + (y[2]+y[3])*log(1-theta.star) + y[4]*log(theta.star))

rej.alg.multiple <- function(n, y=c(125, 18, 20, 34)){
  # Sample from prior distribution g(theta)
  theta <- runif(n,0,1)
  # Sample u from a uniform distribution
  u <- runif(n,0,1)
  # Compute alpha
  f <- (2+theta)^y[1] * (1-theta)^(y[2]+y[3]) * theta^y[4]
  alpha <- 1/c * f
  # Accepted samples and acceptance ratio
  accepted <- theta[u <= alpha]
  ratio <- length(accepted) / n
  
  # Return accepted values and acceptance ratio
  list(accepted = accepted, ratio = ratio)
}
```

## 2.
We denote the posterior mean of $\theta$ by $\mu_{\theta} = \text{E}(f(\theta\mid y))$. Using Monte-Carlo integration, the mean can be estimated as
$$
\hat \mu_{\theta} = \frac1M \sum_{i=1}^M \theta_i,
$$
where $\{\theta_i\}_{i=1}^M$ are sampled from the posterior density $f(\theta\mid y)$. To make it easier to calculate, we implement a version of the algorithm given in part (1) which can sample a specified number of samples.
```{r}
rej.alg.single <- function(n, y=c(125, 18, 20, 34)){
  # Store samples
  samples <- rep(0,n)
  
  i = 1
  while(i<=n){
    # Sample from prior distribution
    theta <- runif(1,0,1)
    # Calculate f
    f <- (2+theta)^y[1] * (1-theta)^(y[2]+y[3]) * theta^y[4]
    # Calculate acceptance probability alpha
    alpha <- 1/c * f
    
    # Check if sample is accepted
    if(runif(1,0,1) <= alpha){
      samples[i] <- theta
      i <- i+1
    }
  }
  # Return samples
  samples
}

# Making 10000 samples from the posterior distribution
M <- 10000
samples <- rej.alg.single(M)

# Monte-Carlo estimate for mean
mu.theta <- mean(samples)
mu.theta
```

Using $M=10000$ samples, Monte-Carlo integration gives the estimate $\hat \mu_{\theta} =$ `r mu.theta`.

```{r}
sim.rej <- data.frame(x = samples)

# Posterior density (up to a multiplicative constant)
f <- function(theta, y=c(125, 18, 20, 34)){
  (2+theta)^y[1] * (1-theta)^(y[2]+y[3]) * theta^y[4]
}

# Calculating the theoretical density and normalizing
theta = seq(from = 0, to = 1, by = 0.005)
norm.const <- integrate(f,0,1)$value
y <- f(theta) / norm.const

exact <- data.frame(x = theta, y = y)


# Histogram + theoretical density + vertical line at estimated mean
ggplot(sim.rej) + 
  geom_histogram(aes(x = x, y = ..density.., fill = "Simulated"),
                 alpha = 0.8, binwidth = 0.01) + 
  geom_line(data = exact , aes(x = x, y = y, color = 'Exact'))  +
  scale_color_manual(name = "", values = c("Exact" = "#e0474c")) +
  scale_fill_manual(name = "", values = c("Simulated" = "#69b3a2")) + 
  geom_vline(xintercept=mu.theta, color="#000000")

```

Below is an estimate of the mean using numerical integration:
```{r}
# Calculating the mean numerically
mu.num <- integrate(function(x) x*f(x)/norm.const, 0, 1)
mu.num$value
```
We see that the estimate of the mean obtained using Monte Carlo integration, $\hat \mu_{\theta} =$ `r mu.theta`, is close to the numerical value $\hat \mu_{\theta}^{\text{(num)}} =$ `r mu.num$value`.

## 3.
To get an estimate for how many generations our algorithm needs on average to produce one sample, we can first find the acceptance ratio using Monte-Carlo simulation. Then the average number of samples needed is equal to the inverse of the acceptance ratio. In addition, a numerical estimate can be carried out by integrating $\int_0^1 \frac{\tilde f(\theta\mid y)}{c\ g(\theta)}\ \text{d}\theta$, so we can see if the Monte Carlo estimate gives a reasonable value.

```{r}
# Estimated acceptance ratio practically
M <- 10000
ratio.est <- rej.alg.multiple(M)$ratio
1/ratio.est

# Numerically calculated acceptance ratio
ratio.num <- integrate(f,0,1)$value / c
1/ratio.num
```
We see that the average number of generations needed to produce one sample are calculated practically as `r 1/ratio.est` and numerically as `r 1/ratio.num`. The practical estimate is close to the numerical value, as expected.

## 4.
Now we assume that the prior density is $g(\theta) = \text{Beta}(1,5) = 5(1-\theta)^4$. The new posterior density then becomes
$$
f_2(\theta\mid y) \propto f(y \mid\theta)\ 5 (1-\theta)^4 \propto 5 (2+\theta)^{y_1}(1-\theta)^{4+ y_2+y_3}\theta^{y_4} \qquad \text{for}\quad \theta \in (0,1),
$$
where we have used the subscript (2) to indicate that this posterior density is different than the one using the uniform distribution. We want to estimate the posterior mean $\mu = \text{E}(\theta\mid y)$ under the new prior density based on the samples obtained in part (2). To do so, we will use importance sampling, where $f_2(\theta\mid y)$ is our target distribution and $f(\theta\mid y)$ is our proposal distribution. An estimate for the posterior mean is thus given as
$$
\hat \mu_{\theta, 2} = \text{E}(\theta \mid y) = \frac{\sum_{i=1}^N \theta_i \frac{f_2(\theta_i \mid y)}{f(\theta_i \mid y)}}{\sum_{i=1}^N \frac{f_2(\theta_i \mid y)}{f(\theta_i \mid y)}} = \frac{\sum_{i=1}^N \theta_i w(\theta_i)}{\sum_{i=1}^N w(\theta_i)},
$$
where the importance weights are $w(\theta_i) = \frac{f_2(\theta_i\mid y)}{f(\theta_i\mid y)} = 5(1-\theta_i)^4$.
```{r}
# Weight function
w <- function(theta) 5*(1-theta)^4

# Calculate new posterior mean
mu.theta2 <- sum(samples*w(samples)) / sum(w(samples))
mu.theta2
```
We see that when using the $\text{Beta}(1,5)$ prior, our estimate for the posterior mean $\hat \mu_{\theta,2} =$ `r mu.theta2` is lower than the estimated posterior mean using the uniform prior. The reason for this is that the densities in $\text{Beta}(1,5)$ is higher in the lower end of the interval $(0,1)$, such that samples with lower values of $\theta$ has higher weights and thus contributes more relative to samples with higher values of $\theta$. Intuitively, this in turn means that the estimate must be lower with this method.

In this particular example we see that the uniform prior got an estimate closer to the numerical value than the $\text{Beta}(1,5)$ prior. This highlights the importance of choosing good priors to get reasonable posterior estimates.

