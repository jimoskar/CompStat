---
title: "TMA4300: Exercise 1"
author: "Jim Totland, Martin Tufte"
date: "1/29/2022"
output: pdf_document
fig_width: 3
fig_height: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 5, fig.height = 4, fig.align = "center")
library(tidyverse)
```



# Problem A
## A.1
The exponential distribution has a cumulative density function given by

$$
F(x) = 1 - e^{-\lambda x},
$$

where $\lambda$ is the rate parameter. By defining $u := F(x)$, $x$ can be expressed as 

$$
x = -\frac{1}{\lambda}\ln(1-u) =: F^{-1}(u).
$$
This means that we can use the \textit{inversion method} to simulate from the exponential distribution. Let $U \sim \mathcal{U}_{[0,1]}$ and calculate $X = F^{-1}(U)$, then $X \sim \mathrm{Exp}(\lambda)$. A function which simulates from the exponential distribution is given below.

```{r}
sim.exp <- function(rate, n){
  # Vector of n uniform values
  u <- runif(n, 0, 1)
  # Note that (1-U) ~ U, so we can use u instead of 1-u in the returned value.
  -1/rate * log(u)
}
```
Next, we need to check if this gives reasonable results by comparing our simulated values to the theoretical knowledge using $\lambda = 0.5$ and $n = 1000$.
```{r, cache=TRUE}
rate <- 0.5
n <- 1000
sim <- data.frame(x = sim.exp(rate, n))
x = seq(from = 0, to = 15, by = 0.1)
exact <- data.frame(x = x, y = rate*exp(-rate*x))

ggplot(sim) + 
  geom_histogram(aes(x = x, y = ..density..),
                 alpha = 0.8, fill = "#69b3a2", binwidth = 0.1) + 
  geom_line(data = exact , aes(x = x, y = y), color = "#e0474c")  +
  geom_label(
    label="Simulated", 
    x=14,
    y=0.5,
    label.padding = unit(0.55, "lines"), # Rectangle size around label
    label.size = 0.35,
    color = "white",
    fill= "#69b3a2"
  ) +
  geom_label(
    label="Exact", 
    x=14,
    y=0.42,
    label.padding = unit(0.55, "lines"), # Rectangle size around label
    label.size = 0.35,
    color = "white",
    fill = "#e0474c"
  ) +
  xlim(0,17)
```
We also compare the estimated mean and variance to first and second central moments of the exponential distribution:

```{r}
# Calculate the cumulative mean
data.frame(n.samples = 1:n,
           mean = cumsum(sim$x)/(1:n)) %>%
  ggplot() + geom_line(aes(n.samples, mean)) +
  geom_hline(yintercept = 1/rate, color = "red") +
  ggtitle("mean")
  
# Calculate the cumulative variance
data.frame(n.samples = 1:n,
           mean = cumsum(sim$x)/(1:n),
           mean2 = cumsum(sim$x^2)/(1:n)) %>%
  mutate(var = mean2-mean^2) %>%
  ggplot() + geom_line(aes(n.samples, var)) +
  geom_hline(yintercept = 1/rate^2, color = "red") +
  ggtitle("variance")
```
We observe that the the sampled mean and variance approach the theoretical values as the number of samples grows larger. 


## A.2

We consider the probability distribution given by
$$
g(x) = 
\begin{cases}
c x^{\alpha-1}, & 0 < x < 1\\
ce^{-x}, & 1 \le x\\
0, & \text{otherwise}
\end{cases},
$$
where $c$ is a normalizing constant and $\alpha \in (0,\ 1)$.

### a)
Normalizing the density using $\int_{-\infty}^{\infty} g(x)\ \text{d}x = c \int_0^1 x^{\alpha-1}\ \text{d}x + c\int_1^{\infty} e^{-x}\ \text{d}x = c(\frac1\alpha + \frac1e) = 1$, it follows that the normalizing constant is $c = (\frac1\alpha + \frac1e)^{-1}$.
The cumulative distribution function is given by
$$
G(x) = \int_{-\infty}^x g(y)\ \text{d}y =
\begin{cases}
c \large\int_0^x y^{\alpha-1}\ \text{d}y, & 0 < x < 1 \\
\dfrac{c}{\alpha} + c \large\int_1^{x} e^{-y}\ \text{d}y, & 1 \le x \\
0, & \text{otherwise}
\end{cases},
$$
which evaluates to
$$
G(x) =
\begin{cases}
\dfrac{cx^\alpha}{\alpha}, & 0 < x < 1 \\
1 - c e^{-x}, & 1 \le x \\
0, & \text{otherwise}
\end{cases}.
$$
The inverse is the unique function $G^{-1}$ such that $x = G^{-1}(u)$. Since $G(x)$ is piecewise continuous and monotonic, we can calculate the inverses for its constituencies and find appropriate intervals. The intervals are simply found from calculating the corresponding boundary points. In $G(x)$ one of these points is located at $x=1$, meaning $G(1) = \frac{c}{\alpha}$ will separate two continuous parts in $G^{-1}$. After solving for $u$, we end up with the inverse given by
$$
G^{-1}(u) =
\begin{cases}
\left(\dfrac{\alpha u}{c}\right)^{-\alpha}, & 0 \le u < \dfrac{c}{\alpha} \\
-\ln\left(\dfrac{1-u}{c}\right), & \dfrac{c}{\alpha} \le u
\end{cases}.
$$

### b)
Function for generating samples from $g$:
```{r}
sim.g <- function(alpha, n){
  # Normalizing constant
  c <- (1/alpha + exp(-1))^-1
  # Vector of n uniform values
  u <- runif(n, 0, 1)
  
  (alpha/c * u)^(1/alpha) * (u < c/alpha) - log((1-u)/c) * (c/alpha <= u)
}
```

```{r, echo = FALSE}
# Implemented for testing purposes

g <- function(alpha, x){
  # Normalizing constant
  c <- (1/alpha + exp(-1))^-1
  c*x^(alpha-1) * (0<x)*(x<1) + c*exp(-x) *(x>=1)
} 

G <- function(alpha, x){
  # Normalizing constant
  c <- (1/alpha + exp(-1))^-1
  c/alpha*x^alpha * (0<x)*(x<1) + (1 - c*exp(-x)) *(x>=1)
} 

G.inv <- function(alpha, u){
  # Normalizing constant
  c <- (1/alpha + exp(-1))^-1
  
  (alpha/c * u)^(1/alpha) * (u < c/alpha) - log((1-u)/c) * (u >= c/alpha)
}
```

To test our implementation we use $\alpha = 0.5$ and simulate for $n=1000$.

```{r}
alpha <- 0.5
n <- 1000
sim <- data.frame(x = sim.g(alpha, n))
x = seq(from = 0.02, to = 5, by = 0.05)
exact <- data.frame(x = x, y = g(alpha, x))

ggplot(sim) + 
  geom_histogram(aes(x = x, y = ..density..),
                 alpha = 0.8, fill = "#69b3a2", binwidth = 0.1) + 
  geom_line(data = exact , aes(x = x, y = y), color = "#e0474c")
```
From the histogram, we see that we are able to generate samples from $g$ corresponding to the true density. Next we find the theoretical mean and variance and compare with the estimated values. The theoretical mean is given by
$$
\mu_g := \int_{-\infty}^{\infty} x\ g(x)\ \text{d}x = c \int_{0}^1 x^{\alpha}\ \text{d}x + c \int_{1}^{\infty} x e^{-x}\ \text{d}x = \frac{c}{\alpha+1} + \frac{2c}{e}.
$$
Similarly, we find the variance
$$
\begin{split}
\text{Var}_g & := \int_{-\infty}^{\infty} x^2 f(x)\ \text{d}x - \mu_g^2 \\
& = c \int_0^1 x^{\alpha + 1}\ \text{d}x + c \int_{1}^{\infty} x^2 e^{-x}\ \text{d}x - \mu_g^2\\
& = \frac{c}{\alpha+2} + \frac{5c}{e} - \left(\frac{c}{\alpha+1} + \frac{2c}{e}\right)^2.
\end{split}
$$
Comparing the true mean and variance with estimated values:
```{r}
alpha <- 0.5
c <- (1/alpha + exp(-1))^-1

# 10000 realizations from simulation
sim <- sim.g(alpha, 10000)

# Theoretical mean and variance
mean.g <- c/(alpha+1) + 2*c/exp(1)
var.g <- c/(alpha+2) + 5*c/exp(1) - mean.g^2

# Estimated mean
mean.est <- mean(sim)
var.est <- var(sim)

# Comparing means
c(mean.g, mean.est)

# Comparing variance
c(var.g, var.est)
```
We see that our simulation gives approximate correct values for the theoretical mean and variance.


## A.3
### a)
To find the value of $c$, we integrate the density over the entire domain and equate the result to 1:
$$
1 = \int_{-\infty}^{\infty}f(x)dx = c\int_{-\infty}^{\infty}\frac{e^{\alpha x}}{(1 + e^{\alpha x})^2}dx.
$$
To progress from here, we introduce the substitution, $v = e^{\alpha x}$, which gives

$$
1 = \frac{c}{\alpha} \int_0^{\infty} \frac{dv}{(1 + v)^2} = \frac{c}{\alpha}(-\frac{1}{1 + v})\Big|_0^\infty =\frac{c}{\alpha}.
$$
Consequently, $c = \alpha$.

### b)
The CDF is defined as follows,
$$
\begin{split}
F(x) &= \int_{-\infty}^xf(z)dz =  \int_0^{\exp(\alpha x)} \frac{dv}{(1 + v)^2} \\
 &= 1-\frac{1}{1 + \exp(\alpha x)} = \frac{\exp(\alpha x)}{1 + \exp(\alpha x)},
\end{split}
$$

where we have used the same substitution as earlier, namely $v = e^{\alpha z}$. We notice that $F(x)$ is the Sigmoid function, which has the well known logit-function as its inverse. I.e.,
$$
F^{-1}(x) = \frac{1}{\alpha}\ln\left(\frac{x}{1-x}\right).
$$

### c)

Since we have an analytic expression for the inverse, we can again use the \textit{inversion method} to sample from $f$. The sampling-function is given below.

```{r}
sim.sigm <- function(alpha, n){
  u <- runif(n,0,1)
  1/alpha * log(u/(1-u))
}
```

```{r}
alpha <- 2
n <- 1000
sim <- data.frame(x = sim.sigm(alpha, n))
x = seq(from = -10, to = 10, by = 0.1)
exact <- data.frame(x = x, y = alpha*exp(alpha*x)/(1 + exp(alpha*x))^2)

ggplot(sim) + 
  geom_histogram(aes(x = x, y = ..density..),
                 alpha = 0.8, fill = "#69b3a2", binwidth = 0.1) + 
  geom_line(data = exact , aes(x = x, y = y), color = "#e0474c")  
```

To compare the simulated values with the theoretical mean and variance, we first need to compute the expression of these moments. \textcolor{red}{Think maybe the mean is undefined??}

## A.4
Below is our implementation of the Box-Muller algorithm.
```{r}
box.mul <- function(n){
  odd <-  FALSE
  if(n %% 2 != 0){
    odd <-  TRUE
    n <- n + 1
  }
  
  x1 <- 2*pi * runif(n/2, 0, 1)
  x2 <- sim.exp(0.5, n/2)
  
  y1 <- sqrt(x2)*cos(x1)
  y2 <- sqrt(x2)*sin(x1)
  
  concat <- c(y1,y2)
  if(odd){
    return(head(concat, -1))
  }
  else{
    return(concat)
  }
}
```
As usual, we compare the results of a simulation against the theoretical distribution.
```{r}
n <- 1000
sim.box.mul <- data.frame(x = box.mul(n))
x = seq(from = -5, to = 5, by = 0.1)
exact <- data.frame(x = x, y = 1/sqrt(2*pi)*exp(-1/2*x^2))

ggplot(sim.box.mul) + 
  geom_histogram(aes(x = x, y = ..density..),
                 alpha = 0.8, fill = "#69b3a2", binwidth = 0.1) + 
  geom_line(data = exact , aes(x = x, y = y), color = "#e0474c")  
```
Finally, we compare the mean and variance resulting from the simulation with the theoretical values. 
```{r}
# mean
data.frame(n.samples = 1:n,
           mean = cumsum(sim.box.mul$x)/(1:n)) %>%
  ggplot() + geom_line(aes(n.samples, mean)) +
  geom_hline(yintercept = 0, color = "red") +
  ggtitle("mean")
  
# variance
data.frame(n.samples = 1:n,
           mean = cumsum(sim.box.mul$x)/(1:n),
           mean2 = cumsum(sim.box.mul$x^2)/(1:n)) %>%
  mutate(var = mean2-mean^2) %>%
  ggplot() + geom_line(aes(n.samples, var)) +
  geom_hline(yintercept = 1, color = "red") +
  ggtitle("variance")
```


## A.5

Let $N_d(\mu, \Sigma)$ be a $d$-variate normal distribution with mean $\mu$ and covariance $\Sigma$. This is our target distribution. To simulate from it, let $x \sim N_d(0,I_d)$ be a standard $d$-variate normal distribution. Then it follows that
$$
y = \mu + A x \quad \implies \quad y \sim N(\mu, AA^T),
$$
where $A$ is a $d \times d$ matrix. Since the covariance matrix $\Sigma$ is both symmetric and positive-definite, we can write $\Sigma = A A^T$ using the Cholesky decomposition. Thus, for a given $\mu$ and $\Sigma$, we are able to simulate from this distribution after simulating from $N_d(0,I_d)$.

Reusing our implementation of the Box-Muller algorithm from A.4, a function for generating realizations from the target is shown below:
```{r}
sim.mvnorm <- function(mu, Sigma, n){
  # Returns n realizations from a d-variate normal distribution
  d <- length(mu)
  
  # Get n d-variate standard normal realizations using the Box-Muller algorithm
  x <- array(box.mul(n*d), dim=c(d,n))
  
  # Cholesky decomposition of Sigma
  A <- t(chol(Sigma)) # Transposes to get a lower triangular matrix
  
  mu + A %*% x
}
```

To test our implementation, we generate a bivariate normal distribution:
```{r}
# Bivariate example
mu <- c(1,2)
Sigma <- matrix(c(1, -.7, -.7, 2), 2)

# Simulate 10000 realizations
y <- sim.mvnorm(mu, Sigma, 10000)

# Plot first 500 points
plot(y[1,1:500], y[2,1:500], main="Bivariate normal distribution", xlab="x", ylab="y", pch=16)
```
Comparing the true mean and covariance with estimated values:
```{r}
# Estimate mean
mu.est <- rowMeans(y)
mu.est

# Estimate covariance matrix
Sigma.est <- cov(t(y))
Sigma.est
```
From the printout in R, we see that the estimated values are close to the true values $\mu = \begin{pmatrix}1 \\ 2\end{pmatrix}$ and $\Sigma = \begin{pmatrix}1 & -0.7 \\ -0.7 & 2\end{pmatrix}$. This means that our function works properly.


\newpage
# Problem B
## B.1
### (a)

## B.2
### (a)
We repeat that 
$$
f^*(x) = \begin{cases} x^{\alpha-1}e^{-x}, \quad &x>0, \\
0, \quad &\text{otherwise}.
\end{cases}
$$
To find $a = \sqrt{\sup_{x\ge 0} f^*(x)}$, we first note that $f^*(x = 0) = 0$ and only consider
$$
\sup_{x > 0} f^*(x),
$$
which amounts to solving

$$
\begin{split}
\frac{d}{dx} f^*(x)= 0 \\
&\iff (\alpha-1)x^{\alpha -2}e^{-x} - x^{\alpha -1}e^{-x} = 0 \\
&\iff x=\alpha-1>0,
\end{split}
$$
which clearly is a global maximum. Consequently,
$$
a = \sqrt{\alpha - 1}.
$$

Analogously, to find  $b_+ = \sqrt{\sup_{x\ge 0} x^2f^*(x)} = \sqrt{\sup_{x\ge 0}g(x)}$, we note that $g(z = 0) = 0$ and only consider $\sup_{x > 0}g(x)$, which amounts to solving

$$
\begin{split}
\frac{d}{dx} g(x)= 0 \\
&\iff (\alpha+1)x^{\alpha}e^{-x} - x^{\alpha + 1}e^{-x} = 0 \\
&\iff x=\alpha + 1>0,
\end{split},
$$
which clearly is a global maximum for $x \ge 0$. Consequently,
$$
b_+ = \sqrt{\alpha + 1}.
$$

Finally, $b_{-} = - \sqrt{\sup_{x \le 0} x^2f^*(x)} = - \sqrt{\sup_{x \le 0} g(x)}$

### (b)

```{r}

alpha <-  1
a <-  sqrt(1-alpha)
b.minus <- 0
b.plus <- sqrt(alpha + 1)

rou.gamma <- function(n, a, b.minus, b.plus, alpha){
  count <-  0
  tries <- 0
  result <- rep(0,n)
  while(count < n){
    x1 <- a * runif(1, 0, 1)
    x2 <- b.minus + b.plus * runif(1, 0, 1) 
    
    if(log(x1) <= 0.5*((alpha - 1)*log(x2/x1) - x2/x1)){
      result[count + 1] = x2/x1
      count <- count + 1
    }
    tries = tries + 1
  }
  return(data.frame(sim = result, tries = tries))
}
```

First, we check that the simulation algorithm gives reasonable results:
```{r}
n <- 5000
alpha <-  3
a <-  sqrt(alpha - 1)
b.minus <- 0
b.plus <- sqrt(alpha + 1)

sim.gamma <- rou.gamma(n, a, b.minus, b.plus, alpha)
x = seq(from = 0.1, to = 12, by = 0.1)
exact <- data.frame(x = x, y = 1/gamma(alpha) * x^(alpha - 1) * exp(-x))

ggplot(sim.gamma) + 
  geom_histogram(aes(x = sim, y = ..density..),
                 alpha = 0.8, fill = "#69b3a2", binwidth = 0.1) + 
  geom_line(data = exact , aes(x = x, y = y), color = "#e0474c") 

```
```{r}
library(pracma)
n <- 1000
alpha.vec <- logseq(1.01, 2000, n = 500)
trial.vec <- rep(0, 100)
for(i in 1:length(alpha.vec)){
  alpha = alpha.vec[i]
  sim.gamma <- rou.gamma(n, a, b.minus, b.plus, alpha)
  trial.vec[i] = sim.gamma$tries[1]
}

plot(log(alpha.vec, base = 10), log(trial.vec, base = 10), ylim = c(2.7,4))
abline(h = 3, col="blue")
```
The number of necessary tries is highest for low values of $\alpha$, and decreases fast, before it stabilizes around $10^{3.2}$. \textcolor{red}{mer tolkning??}

## B.3
We recall that the gamma distribution has probability density function (PDF)

$$
f(x) = \frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha -1}e^{-\beta x}.
$$
We use Marsaglia and Tsang’s method to generate from the gamma distribution:
```{r}
sim.gamma <- function(n, alpha, beta){
  d <- alpha - 1/3
  c <- 1/sqrt(9*d)
  count <- 0
  result <- rep(0, n)
  while(count < n){
    z <- box.mul(1)
    u <- runif(1, 0, 1)
    v <- (1 + c*z)^3
    if(z > -1/c && log(u) < 0.5*z^2 + d - d*V + d*log(V)){
      result[count + 1] = d*v
      count <- count + 1
    }
  }
  return(1/beta * result)
}
```

Check that it works god:...

## B.4
We repeat that $X \sim \text{Gamma}(\alpha, 1)$ and  $Y \sim \text{Gamma}(\beta, 1)$, and note that their joint density is $f_{X,Y}(x,y) = f_X(x)f_Y(y)$, since they are independent. Next, we define $Z = \frac{X}{X + Y}$ and also introduce $V = X + Y$. Then, $X = ZV$ and $Y=V(1-Z)$, and, by the transformation formula, the joint density of $Z$ and $V$ is

$$
\begin{split}
f_{Z, V}(z,v) &= f_{X, Y}(zv, v(1-z)) \cdot \begin{vmatrix} v & z \\
-v & 1-z\end{vmatrix} \\
&= (\Gamma(\alpha) \Gamma(\beta))^{-1}\cdot(zv)^{\alpha-1} e^{-zv} \cdot (v(1-z))^{\beta -1}e^{-v(1-z)}\cdot[v(1-z) + vz] \\
&=(\Gamma(\alpha) \Gamma(\beta))^{-1} \cdot z^{\alpha-1}(1-z)^{\beta - 1}\cdot v^{[\alpha + \beta] - 1} \cdot e^{-v}.
\end{split}
$$
Thus, the marginal distribution of $Z$ is given as

$$
\begin{split}
\int_0^\infty f_{Z, V}(z,v)dv &= \Gamma(\alpha) \Gamma(\beta))^{-1} \cdot z^{\alpha-1}(1-z)^{\beta - 1} \int_0^\infty v^{[\alpha + \beta] - 1} \cdot e^{-v} dv \\
&= \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} z^{\alpha-1}(1-z)^{\beta - 1},
\end{split}
$$
which is what we wanted show.
